{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39209\n",
      "35289\n",
      "3920\n",
      "39209\n",
      "3\n",
      "43\n",
      "x_train (35289, 32, 32, 3)\n",
      "y_train (35289,)\n",
      "x_valid (3920, 32, 32, 3)\n",
      "y_valid (3920,)\n",
      "conv1 (?, 57, 57, 96)\n",
      "maxpool1 (?, 28, 28, 96)\n",
      "conv2 (?, 28, 28, 256)\n",
      "maxpool2 (?, 13, 13, 256)\n",
      "conv3 (?, 13, 13, 384)\n",
      "conv4 (?, 13, 13, 384)\n",
      "conv5 (?, 13, 13, 256)\n",
      "maxpool5 (?, 6, 6, 256)\n",
      "fc6 (?, 4096)\n",
      "fc7 (?, 4096)\n",
      "fc7shape (4096, 43) 4096 43 [(4096, 43)] [[4096   43]] [4096   43]\n",
      "fc8w =  (4096, 43)\n",
      "vd8b =  (43,)\n",
      "probs =  Tensor(\"Softmax_5:0\", shape=(?, 43), dtype=float32)\n",
      "cross_entropy= Tensor(\"Reshape_53:0\", shape=(?,), dtype=float32)\n",
      "loss_operation= Tensor(\"Mean_10:0\", shape=(), dtype=float32)\n",
      "training_operation= name: \"Adam_5\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam_5/update_Variable_94/ApplyAdam\"\n",
      "input: \"^Adam_5/update_Variable_95/ApplyAdam\"\n",
      "input: \"^Adam_5/Assign\"\n",
      "input: \"^Adam_5/Assign_1\"\n",
      "\n",
      "tf.argmax(logits, 1)= Tensor(\"ArgMax_15:0\", shape=(?,), dtype=int64)\n",
      "Training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from alexnet import AlexNet\n",
    "\n",
    "# TODO: Load traffic signs data.\n",
    "training_file = \"train.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "x_list, y_list = train['features'], train['labels']\n",
    "\n",
    "# TODO: Split data into training and validation sets.\n",
    "size_y = y_list.shape[0]\n",
    "valid_size = int(size_y*0.1)\n",
    "train_size = size_y-valid_size\n",
    "n_chn      = x_list[0].shape[2]\n",
    "n_class    = len(np.unique(y_list))\n",
    "AlexNet_size = 227\n",
    "\n",
    "print(size_y)\n",
    "print(train_size)\n",
    "print(valid_size)\n",
    "print(train_size + valid_size)\n",
    "print(n_chn)\n",
    "print(n_class)\n",
    "\n",
    "x_train = x_list[:train_size]\n",
    "x_valid = x_list[train_size:]\n",
    "y_train = y_list[:train_size]\n",
    "y_valid = y_list[train_size:]\n",
    "\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_valid\", x_valid.shape)\n",
    "print(\"y_valid\", y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Define placeholders and resize operation.\n",
    "# x placeholder to be in shape of nonex227x227x3 to match the size of AlexNet\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32,  n_chn))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "resized = tf.image.resize_images(x, (AlexNet_size, AlexNet_size))\n",
    "rate = 0.5\n",
    "one_hot_y  = tf.one_hot(y, n_class)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "\n",
    "# TODO: pass placeholder as first argument to `AlexNet`.\n",
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "# NOTE: `tf.stop_gradient` prevents the gradient from flowing backwards\n",
    "# past this point, keeping the weights before and up to `fc7` frozen.\n",
    "# This also makes training faster, less work to do!\n",
    "fc7 = tf.stop_gradient(fc7)\n",
    "\n",
    "# TODO: Add the final layer for traffic sign classification.\n",
    "# use this shape for the weight matrix (size of fc7 output shape last element and size of n_class)\n",
    "fc7shape = (fc7.get_shape().as_list()[-1], n_class)\n",
    "print(\"fc7shape\", fc7shape, fc7shape[0], fc7shape[-1], [fc7shape], np.array([fc7shape]), np.array(fc7shape))\n",
    "\n",
    "fc8w  = tf.Variable(tf.truncated_normal([fc7shape[0], fc7shape[1]], mean = 0, stddev = 0.01))\n",
    "fc8b  = tf.Variable(tf.zeros(fc7shape[1]))\n",
    "logits= tf.add(tf.matmul(fc7, fc8w), fc8b)\n",
    "probs = tf.nn.softmax(logits)\n",
    "print(\"fc8w = \", fc8w.shape)\n",
    "print(\"vd8b = \", fc8b.shape)\n",
    "print(\"probs = \", probs)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Define loss, training, accuracy operations.\n",
    "# HINT: Look back at your traffic signs project solution, you may\n",
    "# be able to reuse some the code.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "print(\"cross_entropy=\", cross_entropy)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "print(\"loss_operation=\", loss_operation)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "print(\"training_operation=\", training_operation)\n",
    "\n",
    "\n",
    "#evaluation\n",
    "print(\"tf.argmax(logits, 1)=\", tf.argmax(logits, 1))\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data, keep_prob=1):\n",
    "    num_examples = X_data.shape[0]\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_probs:keep_prob})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "\n",
    "# TODO: Train and evaluate the feature extraction model.\n",
    "#Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = x_train.shape[0]\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        #x_strain, y_strain = shuffle(x_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = x_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x:batch_x, y:batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(x_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
